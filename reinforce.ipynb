{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "508586fc-e579-47a7-8a27-20f5464dc221",
   "metadata": {},
   "source": [
    "# REINFORCE for discrete reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb581fa0-34d5-4462-8cd0-b897009cf813",
   "metadata": {},
   "source": [
    "Данный ноутбук содержит эксперимент,\n",
    "включающий в себя использовать дискретную награду\n",
    "    и адаптировать метод [REINFORCE](https://arxiv.org/abs/2402.14740)\n",
    "для такой награды в задаче alignment.\n",
    "\n",
    "RLHF (Reinforcement Learning though Human Feedback) --- метод,\n",
    "предназначен для выравнивания ответов языковой модели с потребностями прользователя.\n",
    "Этот процесс включает в себя следующие этапы:\n",
    "1. SFT (Supervised Fine Tuning) --- обучение с учителем на датасете в формате чата.\n",
    "2. RM (Reward Model) --- модель, предсказывающая награду, которую модель получит при\n",
    "   конкретном запросе и ответе.\n",
    "3. RL (Reinforcement Learning) --- обучение с подкреплением, где в качестве награды используется RM.\n",
    "   Часто в качестве алгоритма выступает [PPO](https://arxiv.org/abs/1707.06347). Однако данный подход\n",
    "   слишком тяжеловесный, поэтому в этом ноутбуке рассматривается метод REINFORCE и его адаптация."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261f8a31-5df9-490b-b2c5-cb186a216f38",
   "metadata": {},
   "source": [
    "__Примечание:__ в этом ноутбуке представлено большое количество строк кода. Однако, для понимания происходящего,\n",
    "достаточно лишь читать содержимое markdown ячеек, а коду обращаться только за уточнениями и дополнительными подробностями."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9105b87c-14c6-4a05-a372-f8d287b4f8d7",
   "metadata": {},
   "source": [
    "## Предварительная подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c54dea-9455-4e44-9e16-61011dabb16d",
   "metadata": {},
   "source": [
    "### Обьявим конфиг\n",
    "\n",
    "Здесь можно изменить batch size, максимальную длину последовательности и другие параметры обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99858aa7-b194-40da-b472-db3d52c0b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"attention_mechanism\": \"flash_attention_2\",\n",
    "    \"cache_dir\": \"data/\",\n",
    "    \"dataset\": \"juyoungml/HelpSteer2-binarized\",\n",
    "    \"DRM_batch_size\": 8,\n",
    "    \"DRM_epochs\": 1,\n",
    "    \"DRM_lr\": 5e-5,\n",
    "    \"DRM_model\": \"data/discrete_reward_model\",\n",
    "    \"DRM_n_classes\": 10,\n",
    "    \"DRM_train\": False,\n",
    "    \"DRL_batch_size\": 12,\n",
    "    \"DRL_epochs\": 1,\n",
    "    \"DRL_lr\": 5e-5,\n",
    "    \"DRL_model\": \"data/discrete_rl_model\",\n",
    "    \"DRL_reward_optimism\": 0.3,\n",
    "    \"DRL_rollout_batch_size\": 1,\n",
    "    \"DRL_train\": False,\n",
    "    \"DRL_warmup_ratio\": 0.03,\n",
    "    \"generation_config\": {\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_k\": 100,\n",
    "        \"max_new_tokens\": 512,\n",
    "    },\n",
    "    \"kl_coef\": 0.1,\n",
    "    \"logger\": \"wandb\",\n",
    "    \"max_token_seq_length\": 2048,\n",
    "    \"RM_batch_size\": 8,\n",
    "    \"RM_epochs\": 1,\n",
    "    \"RM_lr\": 5e-5,\n",
    "    \"RM_model\": \"data/reward_model\",\n",
    "    \"RM_train\": False,\n",
    "    \"RL_batch_size\": 12,\n",
    "    \"RL_epochs\": 1,\n",
    "    \"RL_lr\": 5e-5,\n",
    "    \"RL_model\": \"data/rl_model\",\n",
    "    \"RL_rollout_batch_size\": 1,\n",
    "    \"RL_train\": False,\n",
    "    \"RL_warmup_ratio\": 0.03,\n",
    "    \"SFT_model\": \"HuggingFaceTB/SmolLM2-135M-Instruct\",\n",
    "    \"trainer_dir\": \"data/trainer_output\",\n",
    "    \"wandb_project\": \"discrete_reinforce\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793e9b91-1b12-4a76-abf4-7f751a1352a9",
   "metadata": {},
   "source": [
    "### Необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b4e82e1-523d-45ae-acf9-4457ad0383cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any, Optional\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm, trange\n",
    "import trl\n",
    "import transformers\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f207b3-61b4-41ef-aac6-534afdd93e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = cfg[\"wandb_project\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c107a8dc-90b8-483f-9bc8-47b6bfcad799",
   "metadata": {},
   "source": [
    "### Загружаем датасет\n",
    "\n",
    "В постановке задачи был предложен набор данных `esfrankel17/HelpSteer2_binarized`,\n",
    "однако именно такой набор не существует на Hugging Face,\n",
    "поэтому был выбран набор [`juyoungml/HelpSteer2-binarized`](https://huggingface.co/datasets/juyoungml/HelpSteer2-binarized),\n",
    "на который указывает ссылка из задания.\n",
    "\n",
    "Этот набор уже разбит на тренировочную и валидационную части, размеры которых $7224$ и $373$ соответственно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc680c1c-e6a3-445e-be95-71268a6b2aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected', 'chosen_score', 'rejected_score', 'chosen_rationale', 'rejected_rationale', 'score_diff', 'difficulty'],\n",
       "        num_rows: 7224\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected', 'chosen_score', 'rejected_score', 'chosen_rationale', 'rejected_rationale', 'score_diff', 'difficulty'],\n",
       "        num_rows: 373\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = datasets.load_dataset(cfg[\"dataset\"], cache_dir=cfg[\"cache_dir\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ed8271-f16b-4857-a3b5-928008157417",
   "metadata": {},
   "source": [
    "### Загружаем токенайзер\n",
    "\n",
    "Токенайзер используется от выбранной SFT модели\n",
    "[`HuggingFaceTB/SmolLM2-135M-Instruct`](https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a60db6c7-f7ad-46e4-8596-399ed63193b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(cfg[\"SFT_model\"], cache_dir=cfg[\"cache_dir\"])\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0901f28-a2eb-43b0-868a-13de4cee3bf0",
   "metadata": {},
   "source": [
    "### Преобразуем данные в формат чата\n",
    "\n",
    "SFT модель требует данные в специфичном формате чата.\n",
    "К нему и приводятся запросы и ответы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6b28702-77f5-4723-8b6b-b7dc55ac65c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chat_from_prompt(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        continue_final_message=True,\n",
    "    )\n",
    "\n",
    "def make_chat_from_conversation(prompt, response):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        continue_final_message=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def tokenize_chat(batch):\n",
    "    batch[\"chosen\"] = [make_chat_from_conversation(prompt, response) for prompt, response in zip(batch[\"prompt\"], batch[\"chosen\"])]\n",
    "    batch[\"rejected\"] = [make_chat_from_conversation(prompt, response) for prompt, response in zip(batch[\"prompt\"], batch[\"rejected\"])]\n",
    "    batch[\"prompt\"] = [make_chat_from_prompt(prompt) for prompt in batch[\"prompt\"]]\n",
    "\n",
    "    tokens = tokenizer(batch[\"prompt\"], truncation=True, max_length=cfg[\"max_token_seq_length\"])\n",
    "    batch[\"input_ids\"] = tokens[\"input_ids\"]\n",
    "    batch[\"attention_mask\"] = tokens[\"attention_mask\"]\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15f17c77-9b2c-4695-86fd-2291a0c1536d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb195d2bb4b7482aac8372d3d54bdca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853e820eaa34469a99dda2d1a31b1918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/373 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = data.map(tokenize_chat, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8d0667-15a4-4367-9da6-67be8f01449c",
   "metadata": {},
   "source": [
    "## Continuous Reward Model\n",
    "\n",
    "RM представляет из себя основу от SFT с головой, выдающей один скаляр.\n",
    "Функция потерь выглядит следующим образом:\n",
    "$$\n",
    "\\mathcal{L}_{RM} = -\\log \\sigma (r_{\\psi}(x, y_w) - r_{\\psi}(x, y_l))\n",
    "$$\n",
    "где $x$~--- входной промпт, $y_w$~--- предпочитаемый ответ, $y_l$~--- плохой\n",
    "ответ, а $r_{\\psi}$~--- модель награды.\n",
    "\n",
    "__Примечание:__ в статье про RLOO в определении функции потерь вероятно\n",
    "допущена опечатка, а именно внутри сигмоиды наодится ещё один логарифм.\n",
    "Для подтверждения этого предположения ссылаюсь \n",
    "на реализацию [RewardTrainer](https://github.com/huggingface/trl/blob/main/trl/trainer/reward_trainer.py)\n",
    "и на [Training language models to follow instructions\n",
    "with human feedback](https://arxiv.org/abs/2203.02155)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f6f67d-154f-4d52-92d7-cf0480bf6a7e",
   "metadata": {},
   "source": [
    "### Инициализация Reward Model\n",
    "\n",
    "Требуемая модель загружается с диска, если она уже обучена, или инициализируется\n",
    "весами SFT в противном случае."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95c6cd57-8c33-4f57-a22d-3a303224e770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    }
   ],
   "source": [
    "if cfg[\"RM_train\"]:\n",
    "    rm_model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "        cfg[\"SFT_model\"],\n",
    "        num_labels=1,\n",
    "        attn_implementation=cfg[\"attention_mechanism\"],\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "else:\n",
    "    rm_model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "        cfg[\"RM_model\"],\n",
    "        attn_implementation=cfg[\"attention_mechanism\"],\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518f3f29-c98b-4800-bcd5-f109309f5bf4",
   "metadata": {},
   "source": [
    "### Обучени Reward Model\n",
    "\n",
    "Обучение происходит только в случае необходимости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc5493dd-f3a2-4a75-a545-b30d4b85b89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48eed407f0bc44fe998548629749405f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e06095d0d2475d9a33b2483b85de20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7b7de1410d432189cd8077f17b7f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c8bd5d7d46458ab74989ed34cbd985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/373 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b09b8d47128421495918f0ca4648461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/373 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d38dd350d6418a84655950989ef6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/373 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rm_cfg = trl.RewardConfig(\n",
    "    output_dir=cfg[\"trainer_dir\"],\n",
    "    run_name=\"reward_continuous\",\n",
    "    report_to=cfg[\"logger\"],\n",
    "    eval_strategy=\"epoch\",\n",
    "    max_length=cfg[\"max_token_seq_length\"],\n",
    "    per_device_train_batch_size=cfg[\"RM_batch_size\"],\n",
    "    per_device_eval_batch_size=cfg[\"RM_batch_size\"],\n",
    "    learning_rate=cfg[\"RM_lr\"],\n",
    "    num_train_epochs=cfg[\"RM_epochs\"],\n",
    "    bf16=True\n",
    ")\n",
    "\n",
    "rm_trainer = trl.RewardTrainer(\n",
    "    model=rm_model,\n",
    "    args=rm_cfg,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "768773f0-c9b8-4bd7-96a8-598b7c0fed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg[\"RM_train\"]:\n",
    "    rm_trainer.train()\n",
    "    rm_model.save_pretrained(cfg[\"RM_model\"])\n",
    "    wandb.finish()\n",
    "\n",
    "del rm_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7832911-c449-4c08-bfd6-9d356160a1e7",
   "metadata": {},
   "source": [
    "## Continuous REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be88a84-0655-44d1-84a4-6d524b813ffa",
   "metadata": {},
   "source": [
    "Алгоритм REINFORCE оптимизируется подъёмом по градиенту\n",
    "$$\n",
    "\\mathbb{E}_{x \\sim \\mathcal{D},\\, y \\sim \\pi_{\\theta}(. | x)} [(R(x,y) - b) \\nabla_{\\theta} \\log{\\pi_{\\theta}(y | x)}],\n",
    "$$\n",
    "$$\n",
    "R(x, y) = r_{\\psi}(x, y) - \\beta \\log\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\text{ref}}(y,x)},\n",
    "$$\n",
    "где $\\pi_{\\theta}$, $\\pi_{\\text{ref}}$ --- обучаемая политика и политика, основанная на SFT модели\n",
    "соответственно, $\\beta$ --- гиперпараметр, показвывающий допустимое отклонение обучаемой политики от исходной.\n",
    "В приведенной формуле тажже использовался бейзлайн $b = \\frac{1}{R} \\sum_{i=1}^{S} R(x^i, y^i)$ --- усреднение\n",
    "всех наград за всё время обучения. Он необходим для снижения дисперсии градиента."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161e7ee1-9c4d-422c-bb7c-b7a69b4ed55f",
   "metadata": {},
   "source": [
    "### Реализуем REINFORCETrainer\n",
    "\n",
    "Класс [RLOOTrainer](https://github.com/huggingface/trl/blob/main/trl/trainer/rloo_trainer.py) из \n",
    "библиотеки [trl](https://github.com/huggingface/trl) не подходит, так как \n",
    "эта реализация отличается от описанной в [статье](https://arxiv.org/abs/2402.14740):\n",
    "* Библиотечная реализация использует клипинг для обновления политики,\n",
    "что является избыточным, а его отсутствие не приводит к ухудшению качества.\n",
    "* Преимущество считается отдельно для каждого токена, если не указать\n",
    "параметр `reward_level_kl=False`.\n",
    "* Веса обновляются несколько раз на преимуществах.\n",
    "\n",
    "__Примечание:__ в этой реализация REINFORCE также поддерживает дискретные награды\n",
    "(подробности смотри с соответствующем разделе ноутбука)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5877fde1-fde1-4468-8f06-8cd6f4a728b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceTrainer(transformers.Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        generation_config: dict[str, Any],\n",
    "        ref_model: torch.nn.Module,\n",
    "        reward_model: torch.nn.Module,\n",
    "        kl_coef: float,\n",
    "        rollout_batch_size: int,\n",
    "        reward_optimism: float = 0.5,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.model.generation_config.update(**generation_config)\n",
    "        self.ref_model = ref_model.to(self.model.device)\n",
    "        self.ref_model.eval()\n",
    "        self.reward_model = reward_model.to(self.model.device)\n",
    "        self.reward_model.eval()\n",
    "\n",
    "        self.kl_coef = kl_coef\n",
    "        self.reward_optimism = reward_optimism\n",
    "        self.rollout_batch_size = rollout_batch_size\n",
    "        self.moving_average_reward = 0.0\n",
    "        self.training_steps_counter = 0\n",
    "\n",
    "    def _get_reward_quantile(self, rewards: torch.Tensor) -> torch.Tensor:\n",
    "        np_rewards = rewards.cpu().float().numpy()\n",
    "        cdf = np.cumsum(np_rewards, axis=1)\n",
    "        quantiles = np.zeros(cdf.shape[0])\n",
    "        for i in range(cdf.shape[0]):\n",
    "            quantiles[i] = np.interp(\n",
    "                self.reward_optimism,\n",
    "                cdf[i],\n",
    "                np.arange(cdf.shape[1]),\n",
    "                left=0,\n",
    "                right=cdf.shape[1] - 1\n",
    "            )\n",
    "        return torch.tensor(quantiles, device=rewards.device, dtype=rewards.dtype)\n",
    "        \n",
    "    def _get_reward(self, query_responses: torch.Tensor) -> torch.Tensor:\n",
    "        attention_mask = query_responses != self.processing_class.pad_token_id\n",
    "        rewards = self.reward_model(\n",
    "            input_ids=query_responses,\n",
    "            attention_mask=attention_mask\n",
    "        ).logits\n",
    "        if rewards.ndim == 2 and rewards.shape[1] > 1:\n",
    "            rewards = torch.nn.functional.softmax(rewards, dim=1)\n",
    "            rewards = self._get_reward_quantile(rewards)\n",
    "            \n",
    "        return rewards\n",
    "\n",
    "    def _calculate_log_probs(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        query_responses: torch.Tensor,\n",
    "        query_len: int\n",
    "    ) -> torch.Tensor:\n",
    "        attention_mask = query_responses != model.generation_config.pad_token_id\n",
    "        logits = model(query_responses, attention_mask=attention_mask).logits\n",
    "        logprobs = trl.trainer.utils.selective_log_softmax(\n",
    "            logits[:, :-1], query_responses[:, 1:]\n",
    "        )\n",
    "        return logprobs[:, query_len - 1:].sum(1)\n",
    "\n",
    "    def compute_loss(self, model, inputs, num_items_in_batch=None, return_outputs=False):\n",
    "        queries = inputs[\"input_ids\"]\n",
    "        attention_masks = inputs[\"attention_mask\"]\n",
    "        queries_len = queries.shape[1]\n",
    "        \n",
    "        loss = 0.0\n",
    "        all_rewards = []\n",
    "        all_kl_divergences = []\n",
    "        for _ in range(self.rollout_batch_size):\n",
    "            with torch.no_grad():\n",
    "                query_responses = self.model.generate(\n",
    "                    input_ids=queries,\n",
    "                    attention_mask=attention_masks\n",
    "                )\n",
    "        \n",
    "            logprobs = self._calculate_log_probs(\n",
    "                self.model, query_responses, queries_len\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                ref_logprobs = self._calculate_log_probs(\n",
    "                    self.ref_model, query_responses, queries_len\n",
    "                )\n",
    "\n",
    "                kl_divergence = logprobs.detach() - ref_logprobs\n",
    "                rewards = self._get_reward(query_responses)\n",
    "                rewards_normalized = rewards - self.kl_coef * kl_divergence\n",
    "\n",
    "            all_rewards.append(rewards)\n",
    "            all_kl_divergences.append(kl_divergence)\n",
    "        \n",
    "            self.training_steps_counter += 1\n",
    "            self.moving_average_reward += rewards_normalized.mean().item()\n",
    "            baseline = self.moving_average_reward / self.training_steps_counter\n",
    "            advantages = rewards_normalized - baseline\n",
    "\n",
    "            loss = loss - torch.mean(logprobs * advantages)\n",
    "\n",
    "        loss /= self.rollout_batch_size\n",
    "        self.log({\n",
    "            \"loss\": loss.item(),\n",
    "            \"reward\": torch.stack(all_rewards).mean().item(),\n",
    "            \"kl_divergence\": torch.stack(all_kl_divergences).mean().item(),\n",
    "            \"moving_average_reward\": self.moving_average_reward / self.training_steps_counter,\n",
    "        })\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        return loss\n",
    "\n",
    "    def prediction_step(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        inputs: dict[str, torch.Tensor],\n",
    "        prediction_loss_only: bool,\n",
    "        ignore_keys: Optional[list[str]] = None,\n",
    "    ) -> tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "        queries = inputs[\"input_ids\"]\n",
    "        attention_masks = inputs[\"attention_mask\"]\n",
    "        queries_len = queries.shape[1]\n",
    "        with torch.no_grad():\n",
    "            query_responses = self.model.generate(\n",
    "                input_ids=queries,\n",
    "                attention_mask=attention_masks\n",
    "            )\n",
    "            logprobs = self._calculate_log_probs(\n",
    "                self.model, query_responses, queries_len\n",
    "            )\n",
    "            ref_logprobs = self._calculate_log_probs(\n",
    "                self.ref_model, query_responses, queries_len\n",
    "            )\n",
    "\n",
    "            kl_divergence = logprobs.detach() - ref_logprobs\n",
    "            rewards = self._get_reward(query_responses)\n",
    "            rewards_normalized = rewards - self.kl_coef * kl_divergence\n",
    "        \n",
    "            baseline = self.moving_average_reward / self.training_steps_counter\n",
    "            advantages = rewards_normalized - baseline\n",
    "\n",
    "            loss = -torch.mean(logprobs * advantages)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        return loss, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d54cc-a997-48ff-9fa9-101e232912e8",
   "metadata": {},
   "source": [
    "### Инициализация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40f3c740-ccdb-407a-9560-beb3eb68edad",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_ref_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    cfg[\"SFT_model\"],\n",
    "    attn_implementation=cfg[\"attention_mechanism\"],\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "if cfg[\"RL_train\"]:\n",
    "    rl_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        cfg[\"SFT_model\"],\n",
    "        attn_implementation=cfg[\"attention_mechanism\"],\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "else:\n",
    "    rl_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        cfg[\"RL_model\"],\n",
    "        attn_implementation=cfg[\"attention_mechanism\"],\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5976a73-3981-4d67-a71b-f4b90b54f98b",
   "metadata": {},
   "source": [
    "### Обучение модели\n",
    "\n",
    "Будем использовать SGD в качестве оптимизатора, чтобы сократить объём требуемой памяти.\n",
    "Кроме того, исходя из приведённых в статье формул, в ней использовался именно этот\n",
    "оптимизатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae4efffc-5092-4e33-8c73-db9deaa3f98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_config = transformers.TrainingArguments(\n",
    "    output_dir=cfg[\"trainer_dir\"],\n",
    "    run_name=\"rl_continuous\",\n",
    "    report_to=cfg[\"logger\"],\n",
    "    learning_rate=cfg[\"RL_lr\"],\n",
    "    warmup_ratio=cfg[\"RL_warmup_ratio\"],\n",
    "    per_device_train_batch_size=cfg[\"RL_batch_size\"],\n",
    "    per_device_eval_batch_size=cfg[\"RL_batch_size\"],\n",
    "    num_train_epochs=cfg[\"RL_epochs\"],\n",
    "    eval_strategy=\"epoch\",\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "rl_trainer = ReinforceTrainer(\n",
    "    model=rl_model,\n",
    "    optimizer_cls_and_kwargs=(torch.optim.SGD, {\"lr\": cfg[\"RL_lr\"]}),\n",
    "    args=rl_config,\n",
    "    ref_model=rl_ref_model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    reward_model=rm_model,\n",
    "    generation_config=cfg[\"generation_config\"],\n",
    "    kl_coef=cfg[\"kl_coef\"],\n",
    "    rollout_batch_size=cfg[\"RL_rollout_batch_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bff8b8b4-cbe5-4b50-a7f3-9ce9a16f64f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if cfg[\"RL_train\"]:\n",
    "    rl_trainer.train()\n",
    "    rl_model.save_pretrained(cfg[\"RL_model\"])\n",
    "    wandb.finish()\n",
    "\n",
    "del rl_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb11208c-38d3-43ee-81a2-3842425347b4",
   "metadata": {},
   "source": [
    "### Train Reward\n",
    "![train_reward](imgs/rl_reward.png)\n",
    "\n",
    "### Moving average baseline\n",
    "\n",
    "![rl_baseline](imgs/rl_baseline.png)\n",
    "\n",
    "Можно заметить, что бейзлайн в виде скользящего среднего работает хорошо,\n",
    "постепенно выходя на оптимальную величину.\n",
    "\n",
    "С другой стороны, модель не смогла хорошо обучиться. Это можно объяснить\n",
    "`rollout_batch_size=1`, что приводит к тому, что модель не успевает подстроиться\n",
    "под промпты."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f96f0c-7614-4a79-ab4a-ef0f981ce747",
   "metadata": {},
   "source": [
    "## Discrete Reward Model (DRM)\n",
    "\n",
    "Пусть теперь модель награды возвращает не число, а вероятности классов $r_{\\psi}(y = k)$.\n",
    "Выведем подходящую функцию потерь:\n",
    "$$\n",
    "\\mathcal{L} = -p(y_w > y_l) = -\\sum_{i=1}^K p(y_w = i)p(y_l < i) = -\\sum_{i=1}^K \\sum_{j=1}^{i-1} p(y_w = i)p(y_l = j),\n",
    "$$\n",
    "где $K$ --- количнство классов наград. Для улучшения сходимости (потенциальной), будем\n",
    "оптимизировать $\\log{\\mathcal{L}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cc4faf-bdab-43a5-bde6-267e2e1d0693",
   "metadata": {},
   "source": [
    "### Реализация DRM\n",
    "\n",
    "Для простоты скопируем и слегка модифицируем методы из `RewardTrainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03109b88-eade-4725-be8d-25af38b9870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteRewardTrainer(trl.RewardTrainer):\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        inputs: dict[str, torch.Tensor | Any],\n",
    "        return_outputs=False,\n",
    "        num_items_in_batch=None,\n",
    "    ) -> torch.Tensor | tuple[torch.Tensor, dict[str, torch.Tensor]]:\n",
    "        probs_chosen = torch.nn.functional.softmax(model(\n",
    "            input_ids=inputs[\"input_ids_chosen\"],\n",
    "            attention_mask=inputs[\"attention_mask_chosen\"],\n",
    "            return_dict=True,\n",
    "        )[\"logits\"], 1)\n",
    "        probs_rejected = torch.nn.functional.softmax(model(\n",
    "            input_ids=inputs[\"input_ids_rejected\"],\n",
    "            attention_mask=inputs[\"attention_mask_rejected\"],\n",
    "            return_dict=True,\n",
    "        )[\"logits\"], 1)\n",
    "        \n",
    "        cum_density_rejected = torch.cumsum(probs_rejected, 1) - probs_rejected[:, :1]\n",
    "        loss = -(probs_chosen * cum_density_rejected).sum(1).log().mean()\n",
    "        \n",
    "        if return_outputs:\n",
    "            return loss, {\n",
    "                \"probs_chosen\": probs_chosen,\n",
    "                \"probs_rejected\": probs_rejected,\n",
    "            }\n",
    "        return loss\n",
    "\n",
    "    def prediction_step(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        inputs: dict[str, torch.Tensor | Any],\n",
    "        prediction_loss_only: bool,\n",
    "        ignore_keys: Optional[list[str]] = None,\n",
    "    ) -> tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        if ignore_keys is None:\n",
    "            if hasattr(self.model, \"config\"):\n",
    "                ignore_keys = getattr(self.model.config, \"keys_to_ignore_at_inference\", [])\n",
    "            else:\n",
    "                ignore_keys = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss, probs_dict = self.compute_loss(model, inputs, return_outputs=True)\n",
    "\n",
    "        if prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "\n",
    "        cum_density_rejected = torch.cumsum(probs_dict[\"probs_rejected\"], 1) - probs_dict[\"probs_rejected\"][:, :1]\n",
    "        select_probs = (probs_dict[\"probs_chosen\"] * cum_density_rejected).sum(1)\n",
    "        probs = torch.zeros([select_probs.shape[0], 2])\n",
    "        probs[:, 0] = select_probs\n",
    "        probs[:, 1] = 1 - select_probs\n",
    "\n",
    "        labels = torch.zeros(probs.shape[0])\n",
    "        labels = self._prepare_inputs(labels)\n",
    "        \n",
    "        return loss, probs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a5d75-5bff-4f67-a152-18e58d177899",
   "metadata": {},
   "source": [
    "### Инициализация модеоли"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee6181d4-b861-40a0-9bd2-cf9e915f61bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg[\"DRM_train\"]:\n",
    "    drm_model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "        cfg[\"SFT_model\"],\n",
    "        num_labels=cfg[\"DRM_n_classes\"],\n",
    "        attn_implementation=cfg[\"attention_mechanism\"],\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "else:\n",
    "    drm_model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "        cfg[\"DRM_model\"],\n",
    "        attn_implementation=cfg[\"attention_mechanism\"],\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18602b-c6a2-4f7a-b599-b5620d1a32cc",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea0258cb-5151-4a79-98c8-9bb42a346bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1e64bb33df42a6bb0d3f2418d954bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcfcb1a2375746a99c9beec8b2e52e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4301bf69a0af48d29adbfc7d7942b919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drm_cfg = trl.RewardConfig(\n",
    "    output_dir=cfg[\"trainer_dir\"],\n",
    "    run_name=\"reward_discrete\",\n",
    "    report_to=cfg[\"logger\"],\n",
    "    eval_strategy=\"epoch\",\n",
    "    max_length=cfg[\"max_token_seq_length\"],\n",
    "    per_device_train_batch_size=cfg[\"DRM_batch_size\"],\n",
    "    per_device_eval_batch_size=cfg[\"DRM_batch_size\"],\n",
    "    learning_rate=cfg[\"DRM_lr\"],\n",
    "    num_train_epochs=cfg[\"DRM_epochs\"],\n",
    "    bf16=True\n",
    ")\n",
    "\n",
    "drm_trainer = DiscreteRewardTrainer(\n",
    "    model=drm_model,\n",
    "    args=drm_cfg,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f700da0-2a37-4d76-ad12-dad70aeb14a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg[\"DRM_train\"]:\n",
    "    drm_trainer.train()\n",
    "    drm_model.save_pretrained(cfg[\"DRM_model\"])\n",
    "    wandb.finish()\n",
    "\n",
    "del drm_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5269fc8b-37bb-49d4-bcc2-f25b1b564e14",
   "metadata": {},
   "source": [
    "## REINFORCE с вероятностями дискретных наград\n",
    "\n",
    "Самый простой и очевидный способ интерграции распределения награды в REINFORCE --- вычисление какой-нибудь статистики из распределения\n",
    "и использование её в качестве награды (например среднее или квантиль). Кватнили\n",
    "особенно интересны, так как позволяют регулировать консервативность политики.\n",
    "Например, требуется оптимизировать не награду, которую модель получает в среднем.\n",
    "а некоторую нижнюю границу.\n",
    "$$\n",
    "R(x, y) = S(r_{\\psi}(\\cdot | x, y)) - \\log{\\frac{\\pi_{\\theta}(y|x)}{\\pi_{\\text{ref}}(y|x)}},\n",
    "$$\n",
    "где $S$ --- квантиль распределения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bff24b5-101a-481e-9308-63d7652f1c24",
   "metadata": {},
   "source": [
    "### Инициализация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f93a3d2b-a764-420b-8a47-ca66b94bd254",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg[\"DRL_train\"]:\n",
    "    drl_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        cfg[\"SFT_model\"],\n",
    "        attn_implementation=cfg[\"attention_mechanism\"],\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "else:\n",
    "    drl_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        cfg[\"DRL_model\"],\n",
    "        attn_implementation=cfg[\"attention_mechanism\"],\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f613f211-2f1f-4d0c-ac18-7a833553b2e2",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e082b67-8b4d-42a8-912f-f70d9e62e85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drl_config = transformers.TrainingArguments(\n",
    "    output_dir=cfg[\"trainer_dir\"],\n",
    "    run_name=\"rl_discrete\",\n",
    "    report_to=cfg[\"logger\"],\n",
    "    learning_rate=cfg[\"DRL_lr\"],\n",
    "    warmup_ratio=cfg[\"DRL_warmup_ratio\"],\n",
    "    per_device_train_batch_size=cfg[\"DRL_batch_size\"],\n",
    "    per_device_eval_batch_size=cfg[\"DRL_batch_size\"],\n",
    "    num_train_epochs=cfg[\"DRL_epochs\"],\n",
    "    eval_strategy=\"epoch\",\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "drl_trainer = ReinforceTrainer(\n",
    "    model=drl_model,\n",
    "    optimizer_cls_and_kwargs=(torch.optim.SGD, {\"lr\": cfg[\"DRL_lr\"]}),\n",
    "    args=rl_config,\n",
    "    ref_model=rl_ref_model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    reward_model=drm_model,\n",
    "    generation_config=cfg[\"generation_config\"],\n",
    "    kl_coef=cfg[\"kl_coef\"],\n",
    "    rollout_batch_size=cfg[\"DRL_rollout_batch_size\"],\n",
    "    reward_optimism=cfg[\"DRL_reward_optimism\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f104c30-ffc2-4b6f-8701-63f9c5cec39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg[\"DRL_train\"]:\n",
    "    drl_trainer.train()\n",
    "    drl_model.save_pretrained(cfg[\"DRL_model\"])\n",
    "    wandb.finish()\n",
    "\n",
    "del drl_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c881d1-f7dd-4052-90e7-1c5d7ef24bb6",
   "metadata": {},
   "source": [
    "### Train Reward\n",
    "![drl_train_reward](imgs/drl_reward.png)\n",
    "\n",
    "Аналогичная предущему случаю ситуация."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da14294d-8bf2-4604-863b-3fc158ae58de",
   "metadata": {},
   "source": [
    "## Сравнение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f59b302-6ac6-4ff0-a876-5845d3d97f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def eval_rlhf_model(model, reward_model, dataset):\n",
    "    rewards = []\n",
    "    for i in trange(0, len(dataset), cfg[\"RL_batch_size\"]):\n",
    "        batch = dataset[i : i + cfg[\"RL_batch_size\"]]\n",
    "        tokens = tokenizer.pad(batch)\n",
    "        \n",
    "        output = model.generate(\n",
    "            input_ids=torch.tensor(tokens[\"input_ids\"]).to(model.device),\n",
    "            attention_mask=torch.tensor(tokens[\"attention_mask\"]).to(model.device),\n",
    "        )\n",
    "        reward = reward_model(output).logits\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        del output\n",
    "        torch.cuda.empty_cache()\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbadf69c-a758-49f9-b6b0-ba07c3576d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32084b64993c4a37ae8e7b68b75aa84f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "reward_sft = eval_rlhf_model(rl_ref_model, rm_model, data[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae87f2c-aeb2-44d4-b932-9b191ba5ee45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
